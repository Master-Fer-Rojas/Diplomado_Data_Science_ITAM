# === Importaciones base ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# === Cargar dataset principal ===
# (ajusta el nombre/encoding según tu archivo real)
df = pd.read_csv("datos.csv")

# === Exploración inicial ===
print("INFO:")
print(df.info())
print("\nDESCRIBE:")
print(df.describe(include='all'))

# Conteo de nulos por columna
print("\nNULOS POR COLUMNA:")
print(df.isna().sum().sort_values(ascending=False))

# === Manejo de nulos ===
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(exclude=[np.number]).columns

for c in num_cols:
    if df[c].isna().any():
        df[c] = df[c].fillna(df[c].median())
for c in cat_cols:
    if df[c].isna().any():
        df[c] = df[c].fillna("Desconocido")

# === Conteo de categorías (rápida distribución) ===
for c in cat_cols:
    print(f"\nvalue_counts({c})")
    print(df[c].value_counts(dropna=False).head(15))

# === Histogramas para variables numéricas ===
for c in num_cols:
    plt.figure()
    sns.histplot(df[c], kde=True)
    plt.title(f"Distribución de {c}")
    plt.grid(True)
    plt.show()

# === Barras para categóricas (top 10) ===
for c in cat_cols:
    plt.figure(figsize=(8,4))
    vc = df[c].value_counts().head(10)
    sns.barplot(x=vc.index, y=vc.values)
    plt.title(f"Top categorías en {c}")
    plt.xticks(rotation=45, ha='right')
    plt.grid(True, axis='y')
    plt.show()

# === Features nuevas ===
if {'monto','cantidad'}.issubset(df.columns):
    df['ticket_promedio'] = df['monto'] / df['cantidad'].replace(0, np.nan)

if 'fecha' in df.columns:
    df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')
    df['anio'] = df['fecha'].dt.year
    df['mes']  = df['fecha'].dt.month
    df['dia']  = df['fecha'].dt.day
    df['dow']  = df['fecha'].dt.dayofweek  # lunes=0

# === Top registros por monto ===
if 'monto' in df.columns:
    top10 = df.sort_values('monto', ascending=False).head(10)
    print("\nTOP 10 por monto:")
    print(top10[['monto'] + [c for c in ['fecha','categoria','cliente'] if c in df.columns]])

# === Agregaciones por grupo ===
if {'anio','mes','categoria','monto'}.issubset(df.columns):
    resumen = df.groupby(['anio','mes','categoria'], as_index=False)['monto'].sum()
    print("\nVentas por año-mes-categoría:")
    print(resumen.head())

    mensual = df.groupby(['anio','mes'], as_index=False)['monto'].sum()
    mensual['fecha_mes'] = pd.to_datetime(mensual['anio'].astype(str)+'-'+mensual['mes'].astype(str)+'-01')
    plt.figure(figsize=(10,4))
    sns.lineplot(data=mensual, x='fecha_mes', y='monto')
    plt.title("Ventas mensuales")
    plt.grid(True)
    plt.show()

# === Merge de catálogos externos (si existen) ===
try:
    clientes = pd.read_csv("clientes.csv")
    if 'cliente_id' in df.columns and 'cliente_id' in clientes.columns:
        df = df.merge(clientes, on='cliente_id', how='left')
except FileNotFoundError:
    pass

# === Pivot estilo Excel ===
if {'anio','mes','categoria','monto'}.issubset(df.columns):
    tabla_pivot = pd.pivot_table(df, index=['anio','mes'], columns='categoria', values='monto', aggfunc='sum')
    print("\nPivot (anio-mes x categoría):")
    print(tabla_pivot.head())

# === Filtros con query ===
if {'monto','categoria'}.issubset(df.columns) and len(df):
    ejemplo_cat = df['categoria'].iloc[0] if 'categoria' in df.columns else None
    filtro = df.query("monto > 1000 and categoria == @ejemplo_cat")
    print("\nFiltro ejemplo:")
    print(filtro.head())

# === Duplicados ===
if 'id' in df.columns:
    print("Duplicados por id:", df['id'].duplicated().sum())
    df = df.drop_duplicates(subset=['id'])

# === Casting a categoría (cuando convenga) ===
for c in df.columns:
    if df[c].dtype == 'object' and c != 'fecha':
        try:
            df[c] = df[c].astype('category')
        except Exception:
            pass

# === Series de tiempo ===
if 'fecha' in df.columns and 'monto' in df.columns:
    serie = df.set_index('fecha')['monto'].sort_index()
    serie_m = serie.resample('M').sum()
    roll_3 = serie_m.rolling(3, min_periods=1).mean()
    plt.figure(figsize=(10,4))
    plt.plot(serie_m.index, serie_m.values, label='Mensual')
    plt.plot(roll_3.index, roll_3.values, label='Media móvil 3M')
    plt.legend(); plt.grid(True); plt.show()


# Partimos train/test para evaluar sin hacernos trampa.
# Estandarizamos features para que ninguna variable pese de más.
# Modelo base de clasificación: regresión logística.
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report

# Si existe un target binario en el DataFrame, armamos X/y y probamos un clasificador
targets_bin = [c for c in df.columns if c.lower() in ['target','etiqueta','clase','y']]
X = df.select_dtypes(include=[np.number]).drop(columns=targets_bin, errors='ignore')
y = df[targets_bin[0]] if targets_bin else None

if y is not None and y.nunique() in (2, 3):
    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Escalado
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s  = scaler.transform(X_test)

    # Modelo
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train_s, y_train)
    print("\nAccuracy (LogReg):", clf.score(X_test_s, y_test))

    # Métricas de modelo: ROC/AUC, matriz de confusión, precision/recall/F1.
    if y.nunique() == 2:
        proba = clf.predict_proba(X_test_s)[:, 1]
        auc = roc_auc_score(y_test, proba)
        print("AUC:", auc)
        fpr, tpr, thr = roc_curve(y_test, proba)
        import matplotlib.pyplot as plt
        plt.figure()
        plt.plot(fpr, tpr, label=f"ROC (AUC={auc:.3f})")
        plt.plot([0, 1], [0, 1], '--', lw=1)
        plt.xlabel("FPR"); plt.ylabel("TPR")
        plt.legend(); plt.grid(True); plt.show()

    y_pred = clf.predict(X_test_s)
    print("\nMatriz de confusión:")
    print(confusion_matrix(y_test, y_pred))
    print("\nReporte de clasificación:")
    print(classification_report(y_test, y_pred))

# Regresión lineal para objetivos numéricos.
from sklearn.linear_model import LinearRegression

targets_num = [c for c in df.columns if c.lower() in ['y_num','objetivo','target_num','monto_objetivo']]
if targets_num:
    ynum = df[targets_num[0]]
    Xnum = df.select_dtypes(include=[np.number]).drop(columns=[targets_num[0]], errors='ignore')
    Xtr, Xte, ytr, yte = train_test_split(Xnum, ynum, test_size=0.2, random_state=42)
    lr = LinearRegression()
    lr.fit(Xtr, ytr)
    print("\nR2 (LinearRegression):", lr.score(Xte, yte))

# Top/bottom por monto (si aplica)
if 'monto' in df.columns:
    cols = ['monto'] + [c for c in ['cliente','categoria'] if c in df.columns]
    print("\nTop 5 por monto:")
    print(df.nlargest(5, 'monto')[cols])
    print("\nBottom 5 por monto:")
    print(df.nsmallest(5, 'monto')[cols])

# Dataset sintético 2D para ver fronteras de decisión.
# (sirve para visualizar cómo se ve la separación de clases)
from sklearn.datasets import make_classification
import warnings
warnings.filterwarnings("ignore")

# --- Generar dataset 2D ---
X, y = make_classification(
    n_samples=1000,       # número de instancias
    n_features=2,         # 2D para poder graficar
    n_informative=2,      # features útiles
    n_redundant=0,        # features redundantes
    n_clusters_per_class=1,
    weights=[0.7, 0.3],   # desbalance de clases (70%-30%)
    class_sep=0.8,        # qué tan separables son las clases
    random_state=42
)

# Entrenamos una logística rápida para la demo 2D
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
logit = LogisticRegression()
logit.fit(Xtr, ytr)
print("\nDemo 2D — accuracy:", logit.score(Xte, yte))

# (Continuación) Algo de visualización para la demo 2D sin oversampling
import matplotlib.pyplot as plt
import numpy as np

# Malla para ver frontera de la logística entrenada en la demo 2D (sin ADASYN)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 500),
    np.linspace(y_min, y_max, 500)
)
Z0 = logit.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.figure(figsize=(12, 6))
plt.contourf(xx, yy, Z0, alpha=0.2, cmap=plt.cm.coolwarm)
for label in np.unique(y):
    plt.scatter(X[y==label][:, 0], X[y==label][:, 1], s=15, edgecolor='k', label=f"Clase {label}")
plt.title("Frontera de decisión (demo 2D sin oversampling)")
plt.xlabel("X1"); plt.ylabel("X2")
plt.legend(); plt.grid(True); plt.show()

# Oversampling con ADASYN para balancear clases + frontera de decisión
from imblearn.over_sampling import ADASYN
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# 1) Rebalanceo con ADASYN
adasyn = ADASYN(n_neighbors=5, random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X, y)

# 2) Entrenamiento de modelo en datos rebalanceados
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.3, random_state=42
)
model = LogisticRegression()
model.fit(X_train, y_train)

# 3) Malla para graficar frontera de decisión post-ADASYN
x_min, x_max = X_resampled[:, 0].min() - 1, X_resampled[:, 0].max() + 1
y_min, y_max = X_resampled[:, 1].min() - 1, X_resampled[:, 1].max() + 1
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 500),
    np.linspace(y_min, y_max, 500)
)

# 4) Predicción en cada punto de la malla
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 5) Gráfica final: frontera + puntos rebalanceados
plt.figure(figsize=(15, 7))
plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.coolwarm)  # frontera de decisión
for label in np.unique(y_resampled):
    pts = X_resampled[y_resampled == label]
    plt.scatter(pts[:, 0], pts[:, 1], label=f"Clase {label}", edgecolor="k", s=20)
plt.xlabel("X1")
plt.ylabel("X2")
plt.title("Regresión Logística: Frontera de decisión (ADASYN)")
plt.legend()
plt.grid(True)
plt.show()
                                              
